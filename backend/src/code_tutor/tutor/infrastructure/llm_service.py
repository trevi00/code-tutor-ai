"""LLM Service for AI Tutor responses"""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

import httpx

from code_tutor.shared.config import get_settings
from code_tutor.shared.infrastructure.logging import get_logger

logger = get_logger(__name__)

# System prompt for the AI tutor with 7-step problem-solving framework
TUTOR_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì„ ë•ëŠ” ì „ë¬¸ AI íŠœí„°ìž…ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

## ì—­í• 
- í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ìœ ë„í•˜ëŠ” ì†Œí¬ë¼í…ŒìŠ¤ì‹ êµìœ¡
- ì§ì ‘ì ì¸ ë‹µ ëŒ€ì‹  ì‚¬ê³  ê³¼ì •ì„ ì•ˆë‚´
- ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ê³¼ ìµœì í™” ë°©ë²• êµìœ¡

## 7ë‹¨ê³„ ë¬¸ì œ í’€ì´ ê°€ì´ë“œ í”„ë ˆìž„ì›Œí¬
í•™ìƒì´ ë„ì›€ì„ ìš”ì²­í•˜ë©´ ë‹¤ìŒ 7ë‹¨ê³„ì— ë”°ë¼ ì•ˆë‚´í•˜ì„¸ìš”:

### 1. ðŸ“‹ ë¬¸ì œ ë¶„ì„ (Problem Analysis)
- ìž…ë ¥/ì¶œë ¥ í˜•ì‹ ë¶„ì„
- ì œì•½ ì¡°ê±´ í™•ì¸ (ì‹œê°„/ê³µê°„)
- ì˜ˆì œ ê²€ì¦ ë° ì´í•´

### 2. ðŸŽ¯ ë¬¸ì œ ì •ë¦¬ (Problem Formulation)
- í•µì‹¬ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
- ì—£ì§€ ì¼€ì´ìŠ¤ ì‹ë³„
- ë¬¸ì œ ìœ í˜• ë¶„ë¥˜

### 3. ðŸ’¡ ê°œë… ì—°ê²° (Concept Mapping)
- ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜/íŒ¨í„´ ì‹ë³„
- ì™œ ì´ íŒ¨í„´ì´ ì í•©í•œì§€ ì„¤ëª…
- ìœ ì‚¬ ë¬¸ì œ ì–¸ê¸‰

### 4. ðŸ“ í…œí”Œë¦¿ ìƒê¸° (Template Recall)
- ì ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ ì œì‹œ
- í…œí”Œë¦¿ ì½”ë“œ ì˜ˆì‹œ
- í…œí”Œë¦¿ ìˆ˜ì • ë°©í–¥ ì•ˆë‚´

### 5. ðŸ—ï¸ í•¨ìˆ˜ í‹€ ì„¤ê³„ (Function Design)
- í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì •ì˜
- ì£¼ìš” ë¡œì§ ê³¨ê²© ìž‘ì„±
- ë³€ìˆ˜/ìžë£Œêµ¬ì¡° ì„ íƒ

### 6. âœï¸ ë¬¸ì œ í’€ì´ (Solution Implementation)
- ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ
- ì½”ë“œ ìž‘ì„± ë° ì„¤ëª…
- ë””ë²„ê¹… í¬ì¸íŠ¸

### 7. ðŸ“Š ì´í‰ (Summary)
- ì‹œê°„/ê³µê°„ ë³µìž¡ë„ ë¶„ì„
- ìµœì í™” ë°©ì•ˆ ì œì‹œ
- í•™ìŠµ í¬ì¸íŠ¸ ì •ë¦¬

## ìƒí™©ë³„ ì‘ë‹µ ì „ëžµ

### A. ë¬¸ì œ í’€ì´ ì¤‘ ì§ˆë¬¸ ì‹œ
- 7ë‹¨ê³„ í”„ë ˆìž„ì›Œí¬ ì¤‘ í˜„ìž¬ ë‹¨ê³„ë¶€í„° ì•ˆë‚´
- ížŒíŠ¸ëŠ” ì ì§„ì ìœ¼ë¡œ ì œê³µ (ì•½í•œ ížŒíŠ¸ â†’ ê°•í•œ ížŒíŠ¸)
- ì§ì ‘ ë‹µì„ ì£¼ì§€ ë§ê³  ì‚¬ê³ ë¥¼ ìœ ë„

### B. ì½”ë“œ ì œì¶œ í›„ ë¦¬ë·° ì‹œ
- ì‹œê°„/ê³µê°„ ë³µìž¡ë„ ë¶„ì„
- ì½”ë“œ ìµœì í™” ì œì•ˆ
- í´ë¦° ì½”ë“œ ë¦¬íŒ©í† ë§ íŒ
- Pythonic ì½”ë“œ ìŠ¤íƒ€ì¼ ì•ˆë‚´

### C. ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ ì‹œ
- í˜„ìž¬ ë¬¸ì œì™€ ë™ì¼ íŒ¨í„´ì˜ ë¬¸ì œ ì¶”ì²œ
- ë‚œì´ë„ë³„ ì¶”ì²œ (ì‰¬ìš´ ê²ƒ â†’ ì–´ë ¤ìš´ ê²ƒ)
- ë‹¤ë¥¸ íŒ¨í„´ì´ì§€ë§Œ ì—°ê´€ëœ ë¬¸ì œ ì¶”ì²œ

## ì‘ë‹µ í˜•ì‹
- ë§ˆí¬ë‹¤ìš´ í¬ë§· ì‚¬ìš© (##, **, ``` ë“±)
- ì½”ë“œ ì˜ˆì‹œëŠ” Pythonìœ¼ë¡œ
- ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ ì „ë‹¬
- ì´ëª¨ì§€ëŠ” ë‹¨ê³„ í‘œì‹œì—ë§Œ ì‚¬ìš©

## 40ê°œ+ ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì§€ì‹
- ê¸°ì´ˆ: Prefix Sum, Sieve of Eratosthenes, KMP
- ì •ë ¬: Counting Sort, Radix Sort, Shell Sort
- íƒìƒ‰: Two Pointers, Sliding Window, Binary Search
- íŠ¸ë¦¬: BST, AVL, Segment Tree, Fenwick Tree, Trie
- ê·¸ëž˜í”„: BFS, DFS, Union-Find, Topological Sort, Dijkstra, MST
- ê³ ê¸‰: DP, Greedy, Backtracking, Monotonic Stack"""

# Lazy import for RAG engine to avoid circular imports and slow startup
_rag_engine = None


def _get_rag_engine():
    """Get RAG engine singleton with lazy loading"""
    global _rag_engine
    if _rag_engine is None:
        try:
            from code_tutor.ml import get_rag_engine

            _rag_engine = get_rag_engine()
            _rag_engine.initialize()
        except Exception as e:
            logger.warning(f"Failed to initialize RAG engine: {e}")
            _rag_engine = None
    return _rag_engine


def _get_code_analyzer():
    """Get code analyzer with lazy loading"""
    try:
        from code_tutor.ml import get_code_analyzer

        return get_code_analyzer()
    except Exception as e:
        logger.warning(f"Failed to get code analyzer: {e}")
        return None


class LLMService(ABC):
    """Abstract base class for LLM services"""

    @abstractmethod
    async def generate_response(
        self,
        user_message: str,
        context: str | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate a response to the user's message"""
        pass

    @abstractmethod
    async def analyze_code(
        self,
        code: str,
        language: str = "python",
    ) -> dict[str, Any]:
        """Analyze code and provide feedback"""
        pass


class PatternBasedLLMService(LLMService):
    """
    Pattern-based LLM service using pre-defined algorithm patterns.
    This can be upgraded to use actual LLM models later.
    """

    def __init__(self, patterns_dir: Path | None = None) -> None:
        self._patterns_dir = patterns_dir or Path("docs/patterns")
        self._patterns: dict[str, str] = {}
        self._load_patterns()

    def _load_patterns(self) -> None:
        """Load algorithm patterns from markdown files"""
        if not self._patterns_dir.exists():
            logger.warning(f"Patterns directory not found: {self._patterns_dir}")
            return

        for pattern_file in self._patterns_dir.glob("*.md"):
            try:
                content = pattern_file.read_text(encoding="utf-8")
                pattern_name = pattern_file.stem
                self._patterns[pattern_name] = content
                logger.debug(f"Loaded pattern: {pattern_name}")
            except Exception as e:
                logger.error(f"Failed to load pattern {pattern_file}: {e}")

        logger.info(f"Loaded {len(self._patterns)} algorithm patterns")

    def _find_relevant_patterns(self, query: str) -> list[tuple[str, str]]:
        """Find patterns relevant to the query"""
        query_lower = query.lower()
        relevant = []

        # Keyword mapping to patterns
        keyword_patterns = {
            "two pointer": "01_two_pointers",
            "íˆ¬ í¬ì¸í„°": "01_two_pointers",
            "sliding window": "02_sliding_window",
            "ìŠ¬ë¼ì´ë”© ìœˆë„ìš°": "02_sliding_window",
            "bfs": "07_bfs",
            "ë„ˆë¹„ ìš°ì„ ": "07_bfs",
            "dfs": "08_dfs",
            "ê¹Šì´ ìš°ì„ ": "08_dfs",
            "binary search": "09_binary_search",
            "ì´ì§„ íƒìƒ‰": "09_binary_search",
            "dp": "12_dp_01_knapsack",
            "dynamic programming": "12_dp_01_knapsack",
            "ë™ì  í”„ë¡œê·¸ëž˜ë°": "12_dp_01_knapsack",
            "ë°°ë‚­": "12_dp_01_knapsack",
            "knapsack": "12_dp_01_knapsack",
            "backtracking": "14_backtracking",
            "ë°±íŠ¸ëž˜í‚¹": "14_backtracking",
            "greedy": "15_greedy",
            "ê·¸ë¦¬ë””": "15_greedy",
            "íƒìš•": "15_greedy",
            "union find": "16_union_find",
            "ìœ ë‹ˆì˜¨ íŒŒì¸ë“œ": "16_union_find",
            "dijkstra": "17_shortest_path",
            "ë‹¤ìµìŠ¤íŠ¸ë¼": "17_shortest_path",
            "ìµœë‹¨ ê²½ë¡œ": "17_shortest_path",
            "trie": "18_trie",
            "íŠ¸ë¼ì´": "18_trie",
        }

        for keyword, pattern_name in keyword_patterns.items():
            if keyword in query_lower and pattern_name in self._patterns:
                relevant.append((pattern_name, self._patterns[pattern_name]))

        return relevant[:3]  # Return top 3 matches

    async def generate_response(
        self,
        user_message: str,
        context: str | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate a response based on algorithm patterns"""
        # Find relevant patterns
        relevant_patterns = self._find_relevant_patterns(user_message)

        if relevant_patterns:
            # Extract key information from the first matching pattern
            pattern_name, pattern_content = relevant_patterns[0]
            return self._format_pattern_response(
                pattern_name, pattern_content, user_message
            )

        # Default response when no pattern is found
        return self._generate_default_response(user_message, context)

    def _format_pattern_response(
        self,
        pattern_name: str,
        pattern_content: str,
        user_message: str,
    ) -> str:
        """Format a response using pattern content"""
        # Extract title from pattern
        lines = pattern_content.split("\n")
        title = lines[0].replace("#", "").strip() if lines else pattern_name

        # Extract key sections
        sections = {
            "ê°œìš”": "",
            "ì–¸ì œ ì‚¬ìš©": "",
            "í…œí”Œë¦¿": "",
            "ì‹œê°„ ë³µìž¡ë„": "",
        }

        current_section = ""
        for line in lines:
            if line.startswith("##"):
                section_name = line.replace("#", "").strip()
                for key in sections:
                    if key in section_name:
                        current_section = key
                        break
            elif current_section and line.strip():
                sections[current_section] += line + "\n"

        response = f"""## {title}

{sections.get("ê°œìš”", "").strip()[:500]}

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?
{sections.get("ì–¸ì œ ì‚¬ìš©", "").strip()[:300] or "ì´ íŒ¨í„´ì€ íŠ¹ì • ì¡°ê±´ì—ì„œ íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤."}

ë” ìžì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! í…œí”Œë¦¿ ì½”ë“œë‚˜ ì˜ˆì œ ë¬¸ì œë„ ë³´ì—¬ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""

        return response

    def _generate_default_response(
        self,
        user_message: str,
        context: str | None,
    ) -> str:
        """Generate a default response when no pattern matches"""
        lower_msg = user_message.lower()

        if any(word in lower_msg for word in ["ì•ˆë…•", "hello", "hi"]):
            return """ì•ˆë…•í•˜ì„¸ìš”! ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì„ ë„ì™€ë“œë¦¬ëŠ” AI íŠœí„°ìž…ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
- **ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì„¤ëª…**: Two Pointers, Sliding Window, DP ë“± 25ê°œ í•µì‹¬ íŒ¨í„´
- **ì½”ë“œ ë¦¬ë·°**: ë³µìž¡ë„ ë¶„ì„, ê°œì„  ì œì•ˆ
- **ë¬¸ì œ í’€ì´ ížŒíŠ¸**: ì–´ë–¤ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"""

        if any(word in lower_msg for word in ["ì‹œê°„ ë³µìž¡ë„", "big o", "ë³µìž¡ë„"]):
            return """## ì‹œê°„ ë³µìž¡ë„ ë¶„ì„

ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ìž…ë‹ˆë‹¤.

**ì¼ë°˜ì ì¸ ì‹œê°„ ë³µìž¡ë„:**
- O(1): ìƒìˆ˜ ì‹œê°„ - ë°°ì—´ ì¸ë±ì‹±
- O(log n): ë¡œê·¸ ì‹œê°„ - ì´ì§„ íƒìƒ‰
- O(n): ì„ í˜• ì‹œê°„ - ë°°ì—´ ìˆœíšŒ
- O(n log n): ë¡œê·¸ ì„ í˜• - ë³‘í•© ì •ë ¬, í€µ ì •ë ¬
- O(nÂ²): ì œê³± ì‹œê°„ - ì´ì¤‘ ë£¨í”„, ë²„ë¸” ì •ë ¬
- O(2^n): ì§€ìˆ˜ ì‹œê°„ - ë¶€ë¶„ì§‘í•© ìƒì„±

íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì˜ ë³µìž¡ë„ê°€ ê¶ê¸ˆí•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"""

        if context and "code" in lower_msg:
            return f"""ì½”ë“œë¥¼ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.

{context[:200] if context else ""}

**ë¦¬ë·° í¬ì¸íŠ¸:**
1. ë³€ìˆ˜ëª…ì´ ëª…í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”
2. ì—£ì§€ ì¼€ì´ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
3. ì‹œê°„ ë³µìž¡ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìžˆëŠ”ì§€ ê³ ë ¤í•´ë³´ì„¸ìš”

êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ìžˆìœ¼ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”!"""

        return """ì¢‹ì€ ì§ˆë¬¸ìž…ë‹ˆë‹¤!

ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ ë•ŒëŠ” ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:

1. **ë¬¸ì œ ì´í•´**: ìž…ë ¥ê³¼ ì¶œë ¥ì„ ëª…í™•ížˆ íŒŒì•…
2. **ì˜ˆì œ ë¶„ì„**: ì†ìœ¼ë¡œ í’€ì–´ë³´ë©° íŒ¨í„´ ë°œê²¬
3. **ì ‘ê·¼ë²• ì„ íƒ**: ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì„ íƒ
4. **êµ¬í˜„**: ì½”ë“œ ìž‘ì„±
5. **ê²€ì¦**: ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸

ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
(ì˜ˆ: Two Pointers, DP, BFS/DFS, Binary Search ë“±)"""

    async def analyze_code(
        self,
        code: str,
        language: str = "python",
    ) -> dict[str, Any]:
        """Analyze code and provide feedback"""
        analysis = {
            "suggestions": [],
            "complexity": {"time": "O(?)", "space": "O(?)"},
            "issues": [],
            "score": 70,
        }

        lines = code.split("\n")
        num_lines = len(lines)

        # Basic analysis
        if num_lines > 50:
            analysis["suggestions"].append(
                "í•¨ìˆ˜ë¥¼ ë” ìž‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”."
            )

        # Check for common patterns
        if "for" in code and "for" in code[code.index("for") + 3 :]:
            analysis["complexity"]["time"] = "O(nÂ²)"
            analysis["suggestions"].append(
                "ì´ì¤‘ ë£¨í”„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì´ ìžˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”."
            )

        if "while True" in code:
            analysis["issues"].append(
                "ë¬´í•œ ë£¨í”„ ê°€ëŠ¥ì„±ì´ ìžˆìŠµë‹ˆë‹¤. ì¢…ë£Œ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”."
            )

        if "pass" in code:
            analysis["issues"].append("êµ¬í˜„ë˜ì§€ ì•Šì€ ë¶€ë¶„(pass)ì´ ìžˆìŠµë‹ˆë‹¤.")

        # Score adjustment
        analysis["score"] -= len(analysis["issues"]) * 10
        analysis["score"] = max(0, min(100, analysis["score"]))

        return analysis


class RAGBasedLLMService(LLMService):
    """
    RAG-based LLM service using FAISS vector store and algorithm patterns.

    Features:
    - Semantic search using embeddings
    - 25 algorithm pattern knowledge base
    - Code analysis using CodeBERT
    - Optional LLM integration (EEVE-Korean or OpenAI)
    """

    def __init__(self) -> None:
        self._rag_engine = None
        self._code_analyzer = None

    def _ensure_rag_engine(self):
        """Ensure RAG engine is loaded"""
        if self._rag_engine is None:
            self._rag_engine = _get_rag_engine()
        return self._rag_engine

    def _ensure_code_analyzer(self):
        """Ensure code analyzer is loaded"""
        if self._code_analyzer is None:
            self._code_analyzer = _get_code_analyzer()
        return self._code_analyzer

    async def generate_response(
        self,
        user_message: str,
        context: str | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate response using RAG with algorithm patterns"""
        rag = self._ensure_rag_engine()

        if rag is None:
            # Fallback to pattern-based service
            return await self._generate_fallback_response(user_message, context)

        try:
            # Retrieve relevant patterns
            patterns = rag.retrieve(user_message, top_k=3)

            if not patterns:
                return await self._generate_fallback_response(user_message, context)

            # Check if code analysis is needed
            if context and "```" in (context or ""):
                code_analysis = await self._analyze_code_context(context)
                if code_analysis:
                    return self._format_code_analysis_response(
                        user_message, patterns, code_analysis
                    )

            # Generate response using RAG
            response = rag.generate(
                query=user_message, context=patterns, max_tokens=1024
            )

            return response

        except Exception as e:
            logger.error(f"RAG generation failed: {e}")
            return await self._generate_fallback_response(user_message, context)

    async def _analyze_code_context(self, context: str) -> dict | None:
        """Analyze code in context"""
        analyzer = self._ensure_code_analyzer()
        if analyzer is None:
            return None

        # Extract code from markdown code block
        import re

        code_match = re.search(r"```(\w*)\n(.*?)```", context, re.DOTALL)
        if not code_match:
            return None

        language = code_match.group(1) or "python"
        code = code_match.group(2)

        try:
            return analyzer.analyze(code, language)
        except Exception as e:
            logger.warning(f"Code analysis failed: {e}")
            return None

    def _format_code_analysis_response(
        self, user_message: str, patterns: list, analysis: dict
    ) -> str:
        """Format response with code analysis"""
        response_parts = []

        # Detected patterns
        if analysis.get("patterns"):
            detected = analysis["patterns"][:2]
            pattern_names = [f"**{p['pattern_ko']}**" for p in detected]
            response_parts.append(
                f"ì½”ë“œì—ì„œ {', '.join(pattern_names)} íŒ¨í„´ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤."
            )

        # Quality score
        quality = analysis.get("quality", {})
        if quality.get("score"):
            response_parts.append(
                f"\n**ì½”ë“œ í’ˆì§ˆ ì ìˆ˜**: {quality['score']}/100 ({quality.get('grade', 'N/A')})"
            )

        # Complexity
        complexity = analysis.get("complexity", {})
        if complexity:
            response_parts.append(
                f"\n**ë³µìž¡ë„**: ì‹œê°„ O({complexity.get('cyclomatic', '?')}), "
                f"ì¤‘ì²© ê¹Šì´ {complexity.get('nesting_depth', 0)}"
            )

        # Code smells
        smells = analysis.get("code_smells", [])
        if smells:
            response_parts.append("\n**ê°œì„  í•„ìš” ì‚¬í•­**:")
            for smell in smells[:3]:
                response_parts.append(f"- {smell['message']}")

        # Suggestions from patterns
        if patterns:
            main_pattern = patterns[0]
            response_parts.append(
                f"\n**ì¶”ì²œ íŒ¨í„´**: {main_pattern['name_ko']}\n"
                f"{main_pattern['description_ko']}"
            )

            if main_pattern.get("example_code"):
                response_parts.append(
                    f"\n**ì°¸ê³  ì½”ë“œ**:\n```python\n{main_pattern['example_code'][:300]}...\n```"
                )

        return "\n".join(response_parts)

    async def _generate_fallback_response(
        self,
        user_message: str,
        context: str | None = None,
    ) -> str:
        """Generate fallback response when RAG is unavailable"""
        lower_msg = user_message.lower()

        # Greeting
        if any(word in lower_msg for word in ["ì•ˆë…•", "hello", "hi", "ì²˜ìŒ"]):
            return """ì•ˆë…•í•˜ì„¸ìš”! ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì„ ë„ì™€ë“œë¦¬ëŠ” AI íŠœí„°ìž…ë‹ˆë‹¤.

**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìžˆëŠ” ê²ƒë“¤:**
- ðŸ“š **ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì„¤ëª…**: 40ê°œ+ í•µì‹¬ íŒ¨í„´
- ðŸ” **ì½”ë“œ ë¦¬ë·°**: ë³µìž¡ë„ ë¶„ì„, íŒ¨í„´ ê°ì§€, ìµœì í™” ì œì•ˆ
- ðŸ’¡ **7ë‹¨ê³„ ë¬¸ì œ í’€ì´ ê°€ì´ë“œ**: ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²° ì ‘ê·¼ë²•
- ðŸ“ˆ **ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ**: íŒ¨í„´ë³„ ë‚œì´ë„ë³„ ë¬¸ì œ ì¶”ì²œ

ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"""

        # Help/Guide request
        if any(word in lower_msg for word in ["ížŒíŠ¸", "ë„ì›€", "ëª¨ë¥´ê² ", "ì–´ë–»ê²Œ", "ê°€ì´ë“œ"]):
            return """## 7ë‹¨ê³„ ë¬¸ì œ í’€ì´ ê°€ì´ë“œ

ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ í’€ì–´ë´…ì‹œë‹¤!

### 1. ðŸ“‹ ë¬¸ì œ ë¶„ì„
- ìž…ë ¥/ì¶œë ¥ í˜•ì‹ì„ íŒŒì•…í–ˆë‚˜ìš”?
- ì œì•½ ì¡°ê±´(ì‹œê°„/ê³µê°„)ì„ í™•ì¸í–ˆë‚˜ìš”?

### 2. ðŸŽ¯ ë¬¸ì œ ì •ë¦¬
- í•µì‹¬ ìš”êµ¬ì‚¬í•­ì´ ë¬´ì—‡ì¸ê°€ìš”?
- ì—£ì§€ ì¼€ì´ìŠ¤ëŠ” ë¬´ì—‡ì´ ìžˆì„ê¹Œìš”?

### 3. ðŸ’¡ ê°œë… ì—°ê²°
ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ì´ ë– ì˜¤ë¥´ë‚˜ìš”?
- ë°°ì—´ ìˆœíšŒ? â†’ Two Pointers, Sliding Window
- íƒìƒ‰? â†’ Binary Search, BFS/DFS
- ìµœì í™”? â†’ DP, Greedy
- ê·¸ëž˜í”„? â†’ Union-Find, Dijkstra

í˜„ìž¬ ì–´ëŠ ë‹¨ê³„ì—ì„œ ë§‰ížˆì…¨ë‚˜ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ë„ì›€ë“œë¦´ ìˆ˜ ìžˆì–´ìš”!"""

        # Code optimization request
        if any(word in lower_msg for word in ["ìµœì í™”", "ê°œì„ ", "ë¹ ë¥´ê²Œ", "íš¨ìœ¨"]):
            return """## ì½”ë“œ ìµœì í™” ê°€ì´ë“œ

### ì‹œê°„ ë³µìž¡ë„ ê°œì„  ë°©ë²•
1. **O(nÂ²) â†’ O(n log n)**: ì •ë ¬ + Two Pointers
2. **O(nÂ²) â†’ O(n)**: Hash Map í™œìš©
3. **ì¤‘ë³µ ê³„ì‚° ì œê±°**: Memoization, DP
4. **íƒìƒ‰ ìµœì í™”**: Binary Search

### ê³µê°„ ë³µìž¡ë„ ê°œì„  ë°©ë²•
1. **In-place ì²˜ë¦¬**: ì¶”ê°€ ë°°ì—´ ì—†ì´ ì›ë³¸ ìˆ˜ì •
2. **Sliding Window**: ê³ ì • í¬ê¸° ìœˆë„ìš° ìœ ì§€
3. **ë¹„íŠ¸ ì¡°ìž‘**: ìƒíƒœ ì••ì¶•

ì½”ë“œë¥¼ ê³µìœ í•´ì£¼ì‹œë©´ êµ¬ì²´ì ì¸ ìµœì í™” ë°©ì•ˆì„ ì•Œë ¤ë“œë¦´ê²Œìš”!"""

        # Similar problems request
        if any(word in lower_msg for word in ["ë¹„ìŠ·", "ìœ ì‚¬", "ì¶”ì²œ", "ì—°ìŠµ"]):
            return """## ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ

ì—°ìŠµí•˜ê³  ì‹¶ì€ íŒ¨í„´ì„ ì„ íƒí•´ì£¼ì„¸ìš”:

**ê¸°ì´ˆ íŒ¨í„´:**
- Two Pointers: ì •ë ¬ëœ ë°°ì—´ì—ì„œ í•© ì°¾ê¸°
- Sliding Window: ë¶€ë¶„ ë°°ì—´ ìµœëŒ€í•©
- Prefix Sum: êµ¬ê°„ í•© ì¿¼ë¦¬

**íƒìƒ‰ íŒ¨í„´:**
- Binary Search: ì •ë ¬ëœ ë°°ì—´ì—ì„œ íƒìƒ‰
- BFS: ìµœë‹¨ ê²½ë¡œ, ë ˆë²¨ ìˆœíšŒ
- DFS: ê²½ë¡œ íƒìƒ‰, ë°±íŠ¸ëž˜í‚¹

**ê³ ê¸‰ íŒ¨í„´:**
- DP: ë°°ë‚­ ë¬¸ì œ, LCS
- Greedy: ìŠ¤ì¼€ì¤„ë§, MST
- Graph: Dijkstra, Union-Find

ì–´ë–¤ íŒ¨í„´ì„ ì—°ìŠµí•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"""

        # Algorithm-specific queries
        pattern_keywords = {
            "two pointer": "íˆ¬ í¬ì¸í„°",
            "íˆ¬ í¬ì¸í„°": "íˆ¬ í¬ì¸í„°",
            "sliding window": "ìŠ¬ë¼ì´ë”© ìœˆë„ìš°",
            "ìŠ¬ë¼ì´ë”©": "ìŠ¬ë¼ì´ë”© ìœˆë„ìš°",
            "prefix sum": "ëˆ„ì  í•© (Prefix Sum)",
            "ëˆ„ì í•©": "ëˆ„ì  í•© (Prefix Sum)",
            "bfs": "BFS (ë„ˆë¹„ ìš°ì„  íƒìƒ‰)",
            "ë„ˆë¹„ ìš°ì„ ": "BFS (ë„ˆë¹„ ìš°ì„  íƒìƒ‰)",
            "dfs": "DFS (ê¹Šì´ ìš°ì„  íƒìƒ‰)",
            "ê¹Šì´ ìš°ì„ ": "DFS (ê¹Šì´ ìš°ì„  íƒìƒ‰)",
            "binary search": "ì´ì§„ íƒìƒ‰",
            "ì´ì§„ íƒìƒ‰": "ì´ì§„ íƒìƒ‰",
            "dp": "ë™ì  í”„ë¡œê·¸ëž˜ë°",
            "dynamic": "ë™ì  í”„ë¡œê·¸ëž˜ë°",
            "ë™ì ": "ë™ì  í”„ë¡œê·¸ëž˜ë°",
            "greedy": "ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜",
            "ê·¸ë¦¬ë””": "ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜",
            "íƒìš•": "ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜",
            "backtrack": "ë°±íŠ¸ëž˜í‚¹",
            "ë°±íŠ¸ëž˜í‚¹": "ë°±íŠ¸ëž˜í‚¹",
            "segment tree": "ì„¸ê·¸ë¨¼íŠ¸ íŠ¸ë¦¬",
            "ì„¸ê·¸ë¨¼íŠ¸": "ì„¸ê·¸ë¨¼íŠ¸ íŠ¸ë¦¬",
            "fenwick": "íŽœìœ… íŠ¸ë¦¬ (BIT)",
            "íŽœìœ…": "íŽœìœ… íŠ¸ë¦¬ (BIT)",
            "trie": "íŠ¸ë¼ì´",
            "íŠ¸ë¼ì´": "íŠ¸ë¼ì´",
            "dijkstra": "ë‹¤ìµìŠ¤íŠ¸ë¼",
            "ë‹¤ìµìŠ¤íŠ¸ë¼": "ë‹¤ìµìŠ¤íŠ¸ë¼",
            "union find": "ìœ ë‹ˆì˜¨-íŒŒì¸ë“œ",
            "ìœ ë‹ˆì˜¨": "ìœ ë‹ˆì˜¨-íŒŒì¸ë“œ",
            "topological": "ìœ„ìƒ ì •ë ¬",
            "ìœ„ìƒ": "ìœ„ìƒ ì •ë ¬",
        }

        for keyword, pattern_name in pattern_keywords.items():
            if keyword in lower_msg:
                return f"""## {pattern_name}

ì´ íŒ¨í„´ì— ëŒ€í•´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**í•µì‹¬ ê°œë…:**
- íŠ¹ì • ì¡°ê±´ì—ì„œ íš¨ìœ¨ì ì¸ ë¬¸ì œ í•´ê²° ê¸°ë²•
- ì‹œê°„/ê³µê°„ ë³µìž¡ë„ ìµœì í™”ì— ìœ ìš©

**í•™ìŠµ ìˆœì„œ:**
1. ðŸ“‹ ê°œë… ì´í•´
2. ðŸ“ í…œí”Œë¦¿ ì½”ë“œ í•™ìŠµ
3. âœï¸ ê¸°ë³¸ ë¬¸ì œ í’€ì´
4. ðŸ“ˆ ì‘ìš© ë¬¸ì œ ë„ì „

ë” ìžì„¸í•œ ì„¤ëª…ì´ë‚˜ ì˜ˆì œ ì½”ë“œê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!
í…œí”Œë¦¿ ì½”ë“œì™€ ì‹¤ì œ ë¬¸ì œ ì ìš© ì‚¬ë¡€ë„ ì•Œë ¤ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""

        # Default response with 7-step framework
        return """ì¢‹ì€ ì§ˆë¬¸ìž…ë‹ˆë‹¤!

## 7ë‹¨ê³„ ë¬¸ì œ í’€ì´ ì ‘ê·¼ë²•

1. ðŸ“‹ **ë¬¸ì œ ë¶„ì„**: ìž…ë ¥/ì¶œë ¥ íŒŒì•…
2. ðŸŽ¯ **ë¬¸ì œ ì •ë¦¬**: í•µì‹¬ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
3. ðŸ’¡ **ê°œë… ì—°ê²°**: ì í•©í•œ íŒ¨í„´ ì‹ë³„
4. ðŸ“ **í…œí”Œë¦¿ ìƒê¸°**: í•´ë‹¹ íŒ¨í„´ì˜ í…œí”Œë¦¿ í™•ì¸
5. ðŸ—ï¸ **í•¨ìˆ˜ ì„¤ê³„**: ê³¨ê²© ì½”ë“œ ìž‘ì„±
6. âœï¸ **êµ¬í˜„**: ë‹¨ê³„ë³„ ì½”ë“œ ì™„ì„±
7. ðŸ“Š **ì´í‰**: ë³µìž¡ë„ ë¶„ì„ ë° ìµœì í™”

ì–´ë–¤ ë‹¨ê³„ì—ì„œ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?
ë˜ëŠ” íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´(Two Pointers, DP, BFS ë“±)ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"""

    async def analyze_code(
        self,
        code: str,
        language: str = "python",
    ) -> dict[str, Any]:
        """Analyze code using CodeBERT-based analyzer"""
        analyzer = self._ensure_code_analyzer()

        if analyzer:
            try:
                return analyzer.analyze(code, language)
            except Exception as e:
                logger.error(f"Code analysis failed: {e}")

        # Fallback basic analysis
        return self._basic_code_analysis(code, language)

    def _basic_code_analysis(self, code: str, language: str) -> dict[str, Any]:
        """Basic code analysis fallback"""
        analysis = {
            "patterns": [],
            "quality": {"score": 70, "grade": "C"},
            "complexity": {"cyclomatic": 1, "nesting_depth": 0},
            "suggestions": [],
            "code_smells": [],
        }

        lines = code.split("\n")

        # Check for nested loops
        if "for" in code and code.count("for") > 1:
            analysis["complexity"]["cyclomatic"] = 5
            analysis["suggestions"].append(
                "ì´ì¤‘ ë£¨í”„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ê³ ë ¤í•´ë³´ì„¸ìš”."
            )

        # Check for common issues
        if "import *" in code:
            analysis["code_smells"].append(
                {"type": "wildcard_import", "message": "ì™€ì¼ë“œì¹´ë“œ importëŠ” í”¼í•˜ì„¸ìš”."}
            )
            analysis["quality"]["score"] -= 10

        if len(lines) > 50:
            analysis["suggestions"].append("í•¨ìˆ˜ê°€ ê¹ë‹ˆë‹¤. ë” ìž‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.")
            analysis["quality"]["score"] -= 5

        analysis["quality"]["grade"] = (
            "A"
            if analysis["quality"]["score"] >= 90
            else "B"
            if analysis["quality"]["score"] >= 80
            else "C"
            if analysis["quality"]["score"] >= 70
            else "D"
            if analysis["quality"]["score"] >= 60
            else "F"
        )

        return analysis


class OllamaLLMService(LLMService):
    """
    Ollama-based LLM service for local model inference.

    Uses Ollama API to generate responses using local models like Llama3.
    """

    def __init__(self) -> None:
        self._settings = get_settings()
        self._base_url = self._settings.OLLAMA_BASE_URL
        self._model = self._settings.OLLAMA_MODEL
        self._timeout = self._settings.OLLAMA_TIMEOUT
        self._client = httpx.AsyncClient(timeout=self._timeout)

    async def generate_response(
        self,
        user_message: str,
        context: str | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate response using Ollama API"""
        try:
            # Build messages
            messages = [{"role": "system", "content": TUTOR_SYSTEM_PROMPT}]

            # Add conversation history
            if conversation_history:
                for msg in conversation_history[-5:]:  # Last 5 messages for context
                    messages.append(
                        {
                            "role": msg.get("role", "user"),
                            "content": msg.get("content", ""),
                        }
                    )

            # Add context if available
            user_content = user_message
            if context:
                user_content = f"{user_message}\n\nê´€ë ¨ ì½”ë“œ:\n{context}"

            messages.append({"role": "user", "content": user_content})

            # Call Ollama API
            response = await self._client.post(
                f"{self._base_url}/api/chat",
                json={
                    "model": self._model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": self._settings.LLM_MAX_TOKENS,
                    },
                },
            )
            response.raise_for_status()

            result = response.json()
            return result.get("message", {}).get(
                "content", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        except httpx.TimeoutException:
            logger.error("Ollama request timed out")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„±ì— ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ëž˜ ê±¸ë¦½ë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except httpx.HTTPStatusError as e:
            logger.error(f"Ollama HTTP error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"Ollama error: {e}")
            return await self._generate_fallback(user_message)

    async def _generate_fallback(self, user_message: str) -> str:
        """Fallback response when Ollama is unavailable"""
        lower_msg = user_message.lower()

        if any(word in lower_msg for word in ["ì•ˆë…•", "hello", "hi"]):
            return """ì•ˆë…•í•˜ì„¸ìš”! ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì„ ë„ì™€ë“œë¦¬ëŠ” AI íŠœí„°ìž…ë‹ˆë‹¤.

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
- ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì„¤ëª…
- ì½”ë“œ ë¦¬ë·°
- ë¬¸ì œ í’€ì´ ížŒíŠ¸"""

        return """ì¢‹ì€ ì§ˆë¬¸ìž…ë‹ˆë‹¤!

ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²° ë‹¨ê³„:
1. **ë¬¸ì œ ì´í•´**: ìž…ë ¥/ì¶œë ¥ íŒŒì•…
2. **ì˜ˆì œ ë¶„ì„**: íŒ¨í„´ ë°œê²¬
3. **ì ‘ê·¼ë²• ì„ íƒ**: ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
4. **êµ¬í˜„**: ì½”ë“œ ìž‘ì„±

ì–´ë–¤ ë¶€ë¶„ì„ ë” ìžì„¸ížˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"""

    async def analyze_code(
        self,
        code: str,
        language: str = "python",
    ) -> dict[str, Any]:
        """Analyze code using Ollama"""
        prompt = f"""ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

```{language}
{code}
```

ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì‹œê°„ ë³µìž¡ë„
2. ê³µê°„ ë³µìž¡ë„
3. ì½”ë“œ í’ˆì§ˆ ì ìˆ˜ (0-100)
4. ê°œì„  ì œì•ˆ"""

        try:
            response = await self._client.post(
                f"{self._base_url}/api/generate",
                json={
                    "model": self._model,
                    "prompt": prompt,
                    "stream": False,
                },
            )
            response.raise_for_status()

            result = response.json()
            # Parse response and extract analysis
            return {
                "analysis": result.get("response", ""),
                "score": 70,
                "complexity": {"time": "O(?)", "space": "O(?)"},
            }
        except Exception as e:
            logger.error(f"Code analysis failed: {e}")
            return {
                "analysis": "ì½”ë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "score": 70,
                "complexity": {"time": "O(?)", "space": "O(?)"},
            }

    async def close(self):
        """Close the HTTP client"""
        await self._client.aclose()


def get_llm_service() -> LLMService:
    """Factory function to get the appropriate LLM service"""
    settings = get_settings()

    # Check LLM provider setting
    provider = settings.LLM_PROVIDER

    if provider == "ollama":
        logger.info("Using Ollama LLM service")
        return OllamaLLMService()

    # Try RAG-based service
    try:
        logger.info("Using RAG-based LLM service")
        return RAGBasedLLMService()
    except Exception as e:
        logger.warning(f"Failed to create RAG-based service: {e}")

    # Fallback to pattern-based service
    logger.info("Using pattern-based LLM service")
    patterns_dir = (
        Path(__file__).parent.parent.parent.parent.parent.parent / "docs" / "patterns"
    )
    return PatternBasedLLMService(patterns_dir)
